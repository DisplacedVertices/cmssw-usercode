- run brilcalc on lxplus. set up with
  https://cms-service-lumi.web.cern.ch/cms-service-lumi/brilwsdoc.html

  and git for my working dir with scripts:
  https://github.com/jordantucker/mybrilcalc

- since /afs was unmounted @ lpc, symlinks replaced with copies (2017 and 2018 are UL now):

  2015.json -> /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions15/13TeV/Reprocessing/Cert_13TeV_16Dec2015ReReco_Collisions15_25ns_JSON_Silver_v2.txt
  2016.json -> /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions16/13TeV/ReReco/Final/Cert_271036-284044_13TeV_23Sep2016ReReco_Collisions16_JSON.txt
  2017.json -> /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions17/13TeV/Legacy_2017/Cert_294927-306462_13TeV_UL2017_Collisions17_GoldenJSON.txt
  2018.json -> /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions18/13TeV/PromptReco/Cert_314472-325175_13TeV_PromptReco_Collisions18_JSON.txt

most recent:
  2018.json -> /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions18/13TeV/Legacy_2018/Cert_314472-325175_13TeV_Legacy2018_Collisions18_JSON.txt 

- By hand, remove these:

  319337 LS 48          # offline beamspot in wrong place leads to events with many seed tracks

- "2017p8" is the addition of both jsons, e.g.
  compareJSON.py --or ana_201[78].json > ana_2017p8.json

- "ana_".json are the hlt-menu-ok and prescale-ne-1 dropped versions.
  They are produced by first dumping the by-ls int. lumi csvs with
  brilcalc, with and without the --hltpath argument. See
  drop_prescaled.py for example commands.

  drop_prescaled.py compares the lumi reported with and without the
  HLT filter, since brilcalc applies prescales in the former case.

  (The "official" versions of the by-lumi csv files currently live in
  /uscms/home/ali/nobackup/LLP/CornellCode/mfv_10_6_20/src/JMTucker/MFVNeutralino/test/jsons/json_UL/ . They can change with new
  versions of the brilcalc normtag, but this is more stable lately.)

- "_10pc" or "_1pc" are 10% or 1% slices. They are produced with
  pick10pc.py.

- "ana_avail_" are what condor/crab report say they were able to run
  on.  This is made after the first time condor/crab ntuple is run on
  the default set of datasets (currently MET) and the report is
  scrutinized.  (Can run Tools/test/noop.py for this.)
  Thereafter, compare subsequent runs' reports to this
  file. (See utilities.py.)  For a different set of datasets,
  e.g. /SingleMu, use "ana_avail_*_mu".

- To produce the split-by-era jsons, prepare ds.txt with one dataset
  per line.  Then run split_by_era.sh.  It handles one year at a time.
  Can compare /AOD to /MINIAOD this way to see that all runs made it
  into miniaod.

  When done, use jsonoverlaps to find out if the datasets have
  overlaps. (Has happened before, e.g. for 2018A2 and 3 PromptReco
  https://hypernews.cern.ch/HyperNews/CMS/get/physTools/3610.html .)
  Current JSON files are checked without overlaps.
